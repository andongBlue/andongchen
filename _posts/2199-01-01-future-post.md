---
title: 'Bert与SQUAD'
date: 2021-07-15
permalink: /posts/2012/08/blog-post-4/
tags:
  - NLP
  - Machine reading comprehension
  - Pertrain model
---

# Bert与SQUAD

# 入门NLP

对于一个刚入门NLP的小白来说，一开始看到NLP的任务一定会觉得这个领域的任务是琳琅满目，多种多样的。可能一时间找不到自己的落脚点。这里我先将NLP的任务做一个梳理，让大家有一个清晰的认识。总的来说呢，NLP主要分为五个大类的任务，其任务名称如图：

![picture](./Pos1/Untitled.png)

但是呢，这五个任务只是一个大类，这个大类里面还分为很多小类。比如，文本生成任务(Text Generation)这个大类中就有语言模型(language modeling)，机器翻译(Machine translation)，摘要生成(Summarization)，机器阅读理解(Machine Reading Comprehension)以及对话系统(Dialogue)等小类。

不过，不要怕。对于一个已经在NLP领域学习两年的学生来说，我认为NLP领域的知识是环环相扣，无论你一开始学习的是什么方向，当你做过的东西越来越多，学习的范围越来越大的时候，你会发现无形中已经摸过了NLP的很多任务，将你的学习形成一个闭环。NLP之路，抬头看天(了解全貌)，低头看路(专注于自己任务中)，总有一天会到达高手这一站。

# 从机器阅读理解起步

上面说到，NLP的任务虽多，但是是闭环的。所以说，这一次我们从NLP的机器阅读理解这一站开始起航。对于现阶段来说，机器阅读理解任务就是让我们的模型来做阅读理解的题目。形式就像下面这张图：

![Bert%E4%B8%8ESQUAD%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%AF%B4%E7%9A%84%E6%95%85%E4%BA%8B%20dceb32a787714596b5e7c29573696402/Untitled%201.png](Bert%E4%B8%8ESQUAD%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%AF%B4%E7%9A%84%E6%95%85%E4%BA%8B%20dceb32a787714596b5e7c29573696402/Untitled%201.png)

怎么样，是不是感觉很酷，让我们的模型来做语文阅读理解一样，好似机器有了人类理解的能力。当然，也不能太乐观，现在机器阅读理解任务还在逐步探索阶段。在训练方式上，对于不同机器阅读理解数据集，就会有对这个任务的不同解法。对于机器阅读理解任务，我们将其分为三个类型。难度逐步上升：

- 简单问题：对答案的简单匹配和抽取(SUQAD1.0)
- 复杂问题：加入推理(SQUAD2.0的部分)
- 基于对话的问答系统：自由问答和特定任务场景的问答

我们可以看出，如果答案在文章内可以清晰的找到，那么模型就不需要生成答案，只需要将答案抽取出来就好了，这样的任务是简单的，训练这个任务的数据集就是SQUAD1.0。那么更近一步，从我们自身生活中可以发现，有些阅读理解的问题中是没有答案的，正确的答案就是不回答，这种行为更接近智能，任务也变的更困难，训练这个任务的数据集就是SQUAD2.0。那么在机器阅读理解任务中，对话系统是更难的任务了，因为它的答案要和实时场景相匹配。

# 说说用到的数据集-SQUAD

上面关于机器阅读理解描述中，我们反复提及了SQUAD数据集。对于从事机器阅读理解方向研究的同学来说，这个数据集是非常著名的。

这里我们来详细的说说这个著名的数据集。SQUAD(Stanford Question Answering Dataset)是斯坦福大学通过众包的方式来构建的一个机器阅读理解数据集。本质上，这就是一个大规模的英文阅读理解数据集，现在做和英文的阅读理解相关所有任务，都用它。

## 版本

这个数据集现在有两个版本SQUAD1.0个SQUAD2.0。这里有两个版本的原因是因为有不同的研究场景。具体的研究场景，下面就谈到了。

### **SQUAD1.0**

对于1.0版本的数据集中，这个数据集包含107,785的问题以及对应的536的文章。

**文章来源：**维基百科（Wikipeida）上的一系列文章

**与以前的数据集的区别：**相较于以前的阅读理解数据集，SQUAD更大，包含的文章内容也更多。其具体的形式是，SQUAD的答案是短语或者一段话，而不再是一个单词。答案里面包含的信息增多了，所以任务的难度也增加了。

**特点：**阅读理解的所有答案，都可以在文章中完全可以找到(答案可以从文章中完全copy过来)。并且文中的答案是不能是跨行的。也就是说答案是文章指定的一个区间。所以，SQUAD的答案生成是**抽取式**的。

数据集示例如下：

![Bert%E4%B8%8ESQUAD%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%AF%B4%E7%9A%84%E6%95%85%E4%BA%8B%20dceb32a787714596b5e7c29573696402/Untitled%202.png](Bert%E4%B8%8ESQUAD%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%AF%B4%E7%9A%84%E6%95%85%E4%BA%8B%20dceb32a787714596b5e7c29573696402/Untitled%202.png)

### **SQUAD2.0**

我们可以思考一个问题，在阅读理解中，有时候我们无法通过阅读现有文章的内容回答所有问题。也就是说，有些信息我们是无法通过文章找到的。但是在1.0版本的数据集中，模型可以做到获取问题回答答案，遇到了那些无法回答的问题，模型也会强制给出一个回答，这样的情况显然不够智能。所以在2.0版本的数据集中，增加了50,000条没有答案的问题，通过这个数据集中，希望模型可以学会对于没有答案的问题不作回答。也就是说希望我们的模型要有“知道自己不知道”的能力。

![Bert%E4%B8%8ESQUAD%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%AF%B4%E7%9A%84%E6%95%85%E4%BA%8B%20dceb32a787714596b5e7c29573696402/Untitled%203.png](Bert%E4%B8%8ESQUAD%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%AF%B4%E7%9A%84%E6%95%85%E4%BA%8B%20dceb32a787714596b5e7c29573696402/Untitled%203.png)

2.0版的数据集形式就如上面图片所示，上面的图片中展示的两个问题都是没有答案的，没有答案的问题不回答才是正确的。

总结来说，SQUAD是一个主流的抽取式的英语阅读理解数据集。现在大家都在SQUAD2.0上刷榜。其模型分数超过了人类。

![Bert%E4%B8%8ESQUAD%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%AF%B4%E7%9A%84%E6%95%85%E4%BA%8B%20dceb32a787714596b5e7c29573696402/Untitled%204.png](Bert%E4%B8%8ESQUAD%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%AF%B4%E7%9A%84%E6%95%85%E4%BA%8B%20dceb32a787714596b5e7c29573696402/Untitled%204.png)

通过上面的榜单可以看到，在SQUAD2.0这个数据集中，前五名的模型获得的效果已经远超人类。如果将这些模型做一个分析，可以每个模型里面都装着一个Bert。

# 浅说Bert

Bert这个预训练模型，在2021年的今天应该是无人不知，无人不晓了。作为一个入门的介绍，我想先讲Bert在原论文中是如何被训练的。之后我会介绍SQUAD数据集是如何与Bert结合的。

从结构角度来说，Bert是由Transformer的Encoder(编码器)构成的。通过强大的编码能力，可以将语言映射在一个向量空间中，将单词表示为向量，也就是大家常说的Embedding(词向量)。Bert的所做的就是，输入一个句子，基于任务然后吐出来一个基于训练任务的词向量(embedding)。

知道Bert是什么，那么下面就介绍一下Bert在原论文中的两种训练方式。

## 两个训练方法

### ① Masked LM

![Bert%E4%B8%8ESQUAD%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%AF%B4%E7%9A%84%E6%95%85%E4%BA%8B%20dceb32a787714596b5e7c29573696402.png](Bert%E4%B8%8ESQUAD%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%AF%B4%E7%9A%84%E6%95%85%E4%BA%8B%20dceb32a787714596b5e7c29573696402.png)

将一句话输入之后，随机mask掉一个单词，具体mask的方式就是将那个词替换为[MASK]这个符号，然后再mask位置的输出接到一个简单的线性分类器当中。我们希望的是一个简单的线性分类器可以得到正确的答案。如果简单的分类器可以输出正确的答案，就说明这个embedding(词向量)的效果相当的好

### ② Next Sentence Prediction

预测输入的两个句子是不是一句话

![Bert%E4%B8%8ESQUAD%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%AF%B4%E7%9A%84%E6%95%85%E4%BA%8B%20dceb32a787714596b5e7c29573696402%201.png](Bert%E4%B8%8ESQUAD%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%AF%B4%E7%9A%84%E6%95%85%E4%BA%8B%20dceb32a787714596b5e7c29573696402%201.png)

输入两句话，然后Bert输出的是单词的embedding(词向量)。这时从图中可以看出，有两个特殊的输入单词———SEP和CLS。SEP这个单词的意思就是告诉Bert，左右的两个句子是分开的。CLS这个单词的意思就是告诉Bert，这里是要做一个分类任务。然后将这个CLS输出的embedding放入一个简单的分类器中(simple linear)来预测两个句子是不是一句话。如何可以分辨的很好，说明了Bert对于语句相似性有很好的的表示效果。

在Bert的完整训练过程中，这两个训练任务是都要有。这样可以训练出性能优秀的Bert。

在Bert里为了完成不同的任务，设计了不同的特殊单词。这里顺便做一下总结：

- `[CLS]`：告诉模型要做分类任务，其中最后一层的第一个embedding作为分类任务的presention。
- `[SEP]`：告诉Bert左右两边的输入是不同的。
- `[UNK]`：没出现在Bert字典里的字会被这个单词替代。
- `[PAD]`：zero padding，将长度不同的序列补充为固定长度，方便做batch运算。
- `[MASK]`：未知遮罩

## 说回Bert与SQUAD

现在我们已经知道了SQUAD这个数据集以及模型Bert。现在我就可以通过Bert和SQUAD来做机器阅读理解这个任务了。

那么接下来详细说一说在Bert中，如何在SQUAD上解决阅读理解这个问题的。

在原始的Bert任务中，就已经利用SQUAD来做阅读理解任务了。它使用了SEP的这个特殊单词，将Qury(问题)和Document(文章)一起作为输入。然后在Bert中获取良好的embedding(词向量)，然后将这个embedding(词向量)的结果接入一个分类器，分别得到答案在文章中位置的id和结束位置的id。因为SQUAD数据集中的答案是可以直接在文章中抽取出来，所以得到答案起始位置的id和结束位置的id可以直接抽取出正确的答案。

我们使用文章一开始那个例子给大家举例。当我将文章和问题输入给Bert之后，将Bert输出的Embedding(词向量)接入到一个阅读理解任务的模型中(这个模型可以先忽略，对于Bert来说，不同的任务会不同的模型来辅助)。我们发现，输出的结果是'雪'和‘藻’在文本中的位置65和67。然后我们将65-67这三个字抽取出来就得到了答案‘雪衣藻’。

![Bert%E4%B8%8ESQUAD%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%AF%B4%E7%9A%84%E6%95%85%E4%BA%8B%20dceb32a787714596b5e7c29573696402%202.png](Bert%E4%B8%8ESQUAD%E4%B8%8D%E5%BE%97%E4%B8%8D%E8%AF%B4%E7%9A%84%E6%95%85%E4%BA%8B%20dceb32a787714596b5e7c29573696402%202.png)

对于英文的SQUAD数据集，我们的做法和上面一模一样。

那么对于SQUAD2.0数据集来说，这个数据集中有一些没有答案的问题。我们对于这样的问题解法其实和上面没有任何区别，如果我们获得起始位置id比结束位置id大的情况，那么这种不合理的输出，我们就认为这个问题没有答案。

# 结尾

这篇文章中，我先是介绍了NLP的基本任务。然后以SQUAD数据集为中心，介绍了机器阅读理解任务的一些分类，知道抽取式任务是简单的，而问答任务是困难的。最后我以Bert为例介绍了，SQUAD数据集在Bert模型上是怎么解的。

